{"0": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": "Aggregator service consists of three main parts: . | Consumer that reads (consumes) Insights OCP messages from specified message broker. Usually Kafka broker is used but it might be possible to develop a interface for different broker. Insights | OCP messages are basically encoded in JSON and contain results generated by rule engine. | HTTP or HTTPS server that exposes REST API endpoints that can be used to read list of organizations, list of clusters, read rules results for selected cluster etc. Additionally, basic metrics are exposed as well. Those metrics is configured to be consumed by Prometheus and visualized by Grafana. | Storage backend which is some instance of SQL database. Currently SQLite3 and PostgreSQL are fully supported, but more SQL databases might be added later. | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/architecture.html",
    "relUrl": "/architecture.html"
  },"1": {
    "doc": "Architecture",
    "title": "Whole data flow",
    "content": ". | Event about new data from insights operator is consumed from Kafka. That event contains (among other things) URL to S3 Bucket | Insights operator data is read from S3 Bucket and Insights rules are applied to that data | Results (basically organization ID + cluster name + insights results JSON) are stored back into Kafka, but into different topic | That results are consumed by Insights rules aggregator service that caches them | The service provides such data via REST API to other tools, like OpenShift Cluster Manager web UI, OpenShift console, etc. | . Optionally, an organization allowlist can be enabled by the configuration variable enable_org_allowlist, which enables processing of a .csv file containing organization IDs (path specified by the config variable org_allowlist) and allows report processing only for these organizations. This feature is disabled by default, and might be removed altogether in the near future. NOTE . Detailed information about the exact format of consumed data from Kafka topic is available at . https://redhatinsights.github.io/insights-data-schemas/ccx_ocp_results_topic.html . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/architecture.html#whole-data-flow",
    "relUrl": "/architecture.html#whole-data-flow"
  },"2": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": "Authentication is working through x-rh-identity token which is provided by 3scale. x-rh-identity is base64 encoded JSON, that includes data about user and organization, like: . { \"identity\": { \"account_number\": \"0369233\", \"type\": \"User\", \"user\": { \"username\": \"jdoe\", \"email\": \"jdoe@acme.com\", \"first_name\": \"John\", \"last_name\": \"Doe\", \"is_active\": true, \"is_org_admin\": false, \"is_internal\": false, \"locale\": \"en_US\" }, \"internal\": { \"org_id\": \"3340851\", \"auth_type\": \"basic-auth\", \"auth_time\": 6300 } } } . If aggregator didn’t get identity token or got invalid one, then it returns error with status code 403 - Forbidden. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/authentication.html",
    "relUrl": "/authentication.html"
  },"3": {
    "doc": "CI",
    "title": "CI",
    "content": "Travis CI is configured for this repository. Several tests and checks are started for all pull requests: . | Unit tests that use the standard tool go test. | go fmt tool to check code formatting. That tool is run with -s flag to perform following transformations | go vet to report likely mistakes in source code, for example suspicious constructs, such as Printf calls whose arguments do not align with the format string. | golint as a linter for all Go sources stored in this repository | gocyclo to report all functions and methods with too high cyclomatic complexity. The cyclomatic complexity of a function is calculated according to the following rules: 1 is the base complexity of a function +1 for each if, for, case, &amp;&amp; or || Go Report Card warns on functions with cyclomatic complexity &gt; 9 | goconst to find repeated strings that could be replaced by a constant | gosec to inspect source code for security problems by scanning the Go AST | ineffassign to detect and print all ineffectual assignments in Go code | errcheck for checking for all unchecked errors in go programs | shellcheck to perform static analysis for all shell scripts used in this repository | abcgo to measure ABC metrics for Go source code and check if the metrics does not exceed specified threshold | . Please note that all checks mentioned above have to pass for the change to be merged into master branch. History of checks performed by CI is available at RedHatInsights / insights-results-aggregator. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/ci.html",
    "relUrl": "/ci.html"
  },"4": {
    "doc": "Configuration",
    "title": "Configuration",
    "content": " ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/configuration.html",
    "relUrl": "/configuration.html"
  },"5": {
    "doc": "Configuration",
    "title": "Table of contents",
    "content": ". | Clowder configuration | Broker configuration . | About timeout definition | . | Server configuration | CloudWatch configuration | Metrics configuration | . Configuration is done by toml config, default one is config.toml in working directory, but it can be overwritten by INSIGHTS_RESULTS_AGGREGATOR_CONFIG_FILE env var. Also each key in config can be overwritten by corresponding env var. For example if you have config . [storage] db_driver = \"sqlite3\" sqlite_datasource = \"./aggregator.db\" pg_username = \"user\" pg_password = \"password\" pg_host = \"localhost\" pg_port = 5432 pg_db_name = \"aggregator\" pg_params = \"\" . and environment variables . INSIGHTS_RESULTS_AGGREGATOR__STORAGE__DB_DRIVER=\"postgres\" INSIGHTS_RESULTS_AGGREGATOR__STORAGE__PG_PASSWORD=\"your secret password\" . the actual driver will be postgres with password “your secret password” . It’s very useful for deploying docker containers and keeping some of your configuration outside of main config file(like passwords). Clowder configuration . In Clowder environment, some configuration options are injected automatically. Currently Kafka broker configuration is injected this side. To test this behavior, it is possible to specify path to Clowder-related configuration file via AGG_CONFIG environment variable: . export ACG_CONFIG=\"clowder_config.json\" . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/configuration.html#table-of-contents",
    "relUrl": "/configuration.html#table-of-contents"
  },"6": {
    "doc": "Configuration",
    "title": "Broker configuration",
    "content": "Broker configuration is in section [broker] in config file . [broker] address = \"localhost:9092\" timeout = \"30s\" topic = \"topic\" payload_tracker_topic = \"payload-tracker-topic\" service_name = \"insights-results-aggregator\" group = \"aggregator\" enabled = true save_offset = true . | address is an address of kafka broker (DEFAULT: “”) | timeout is the time used as timeout for the Kafka client networking side. See notes above | topic is a topic to consume messages from (DEFAULT: “”) | payload_tracker_topic is a topic to which messages for the Payload Tracker are published (see producer package) (DEFAULT: “”) | service_name is the name of this service as reported to the Payload Tracker (DEFAULT: “”) | group is a kafka group (DEFAULT: “”) | enabled is an option to turn broker on (DEFAULT: false) | save_offset is an option to turn on saving offset of successfully consumed messages. The offset is stored in the same kafka broker. If it turned off, consuming will be started from the most recent message (DEFAULT: false) | . Option names in env configuration: . | address - INSIGHTS_RESULTS_AGGREGATOR__BROKER__ADDRESS | timeout - INSIGHTS_RESULTS_AGGREGATOR__BROKER__TIMEOUT | topic - INSIGHTS_RESULTS_AGGREGATOR__BROKER__TOPIC | payload_tracker_topic - INSIGHTS_RESULTS_AGGREGATOR__BROKER__PAYLOAD_TRACKER_TOPIC | service_name - INSIGHTS_RESULTS_AGGREGATOR__BROKER__SERVICE_NAME | group - INSIGHTS_RESULTS_AGGREGATOR__BROKER__GROUP | enabled - INSIGHTS_RESULTS_AGGREGATOR__BROKER__ENABLED | save_offset - INSIGHTS_RESULTS_AGGREGATOR__BROKER__SAVE_OFFSET | . About timeout definition . The timeout configuration should be an string that can be parsed by the function time.ParseDuration from Golang standard library. This timeout will be applied as the configuration for dial, read and write timeouts of the Sarama Kafka library. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/configuration.html#broker-configuration",
    "relUrl": "/configuration.html#broker-configuration"
  },"7": {
    "doc": "Configuration",
    "title": "Server configuration",
    "content": "Server configuration is in section [server] in config file. [server] address = \":8080\" api_prefix = \"/api/v1/\" api_spec_file = \"openapi.json\" debug = true auth = true auth_type = \"xrh\" maximum_feedback_message_length = 255 . | address is host and port which server should listen to | api_prefix is prefix for RestAPI path | api_spec_file is the location of a required OpenAPI specifications file | debug is developer mode that enables some special API endpoints not used on production. In production, false is used every time. | auth turns on or turns authentication. Please note that this option can be set to false only in devel environment. In production, true is used every time. | auth_type set type of auth, it means which header to use for auth x-rh-identity or Authorization. Can be used only with auth = true. Possible options: jwt, xrh | maximum_feedback_message_length is a maximum possible length of a string for user’s feedback | . Please note that if auth configuration option is turned off, not all REST API endpoints will be usable. Whole REST API schema is satisfied only for auth = true. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/configuration.html#server-configuration",
    "relUrl": "/configuration.html#server-configuration"
  },"8": {
    "doc": "Configuration",
    "title": "CloudWatch configuration",
    "content": "CloudWatch configuration is in section [cloudwatch] in config file . [cloudwatch] aws_access_id = \"a key id\" aws_secret_key = \"tshhhh it is a secret\" aws_session_token = \"\" aws_region = \"us-east-1\" log_group = \"platform-dev\" stream_name = \"insights-results-aggregator\" debug = false . | aws_access_id is an aws access id | aws_secret_key is an aws secret key | aws_session_token is an aws session token | aws_region is an aws region | log_group is a log group for aws logging | stream_name is a stream name for aws logging. If you’re deploying multiple pods, you can add $HOSTNAME to the stream name so that they aren’t writing to the same stream at once | debug is an option to enable debug output of cloudwatch logging | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/configuration.html#cloudwatch-configuration",
    "relUrl": "/configuration.html#cloudwatch-configuration"
  },"9": {
    "doc": "Configuration",
    "title": "Metrics configuration",
    "content": "Metrics configuration is in section [metrics] in config file . [metrics] namespace = \"mynamespace\" . | namespace if defined, it is used as Namespace argument when creating all the Prometheus metrics exposed by this service. | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/configuration.html#metrics-configuration",
    "relUrl": "/configuration.html#metrics-configuration"
  },"10": {
    "doc": "Database",
    "title": "Database",
    "content": "Aggregator is configured to use SQLite3 DB by default, but it also supports PostgreSQL. In CI and QA environments, the configuration is overridden by environment variables to use PostgreSQL. To establish connection to the PostgreSQL instance provided by the minimal stack in docker-compose.yml for local setup, the following configuration options need to be changed in storage section of config.toml: . [storage] db_driver = \"postgres\" pg_username = \"user\" pg_password = \"password\" pg_host = \"localhost\" pg_port = 55432 pg_db_name = \"aggregator\" pg_params = \"sslmode=disable\" . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/database.html",
    "relUrl": "/database.html"
  },"11": {
    "doc": "Database",
    "title": "Migration mechanism",
    "content": "This service contains an implementation of a simple database migration mechanism that allows semi-automatic transitions between various database versions as well as building the latest version of the database from scratch. The migrations are no longer performed during initialization of the service to make running multiple instances of the service in parallel against the same database safer. Instead, the database migration version can now be set using the built-in CLI sub-command migration (aliases: migrations and migrate). Printing information about database migrations ./insights-results-aggregator migrations . Upgrading the database to the latest available migration ./insights-results-aggregator migration latest . Downgrading to the base (empty) database migration version ./insights-results-aggregator migration 0 . Before using the migration mechanism, it is first necessary to initialize the migration information table migration_info. This can be done using the migration.InitInfoTable(*sql.DB) function. Any attempt to get or set the database version without initializing this table first will result in a no such table: migration_info error from the SQL driver. New migrations must be added manually into the code, because it was decided that modifying the list of migrations at runtime is undesirable. To migrate the database to a certain version, in either direction (both upgrade and downgrade), use the migration.SetDBVersion(*sql.DB, migration.Version) function. To upgrade the database to the highest available version, use migration.SetDBVersion(db, migration.GetMaxVersion()). This will automatically perform all the necessary steps to migrate the database from its current version to the highest defined version. See /migration/migration.go documentation for an overview of all available DB migration functionality. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/database.html#migration-mechanism",
    "relUrl": "/database.html#migration-mechanism"
  },"12": {
    "doc": "DB structure",
    "title": "DB structure",
    "content": " ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/db_structure.html",
    "relUrl": "/db_structure.html"
  },"13": {
    "doc": "DB structure",
    "title": "Table of contents",
    "content": ". | Table report | Tables rule and rule_error_key | Table cluster_rule_user_feedback | Table cluster_rule_toggle | Table consumer_error | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/db_structure.html#table-of-contents",
    "relUrl": "/db_structure.html#table-of-contents"
  },"14": {
    "doc": "DB structure",
    "title": "Table report",
    "content": "This table is used as a cache for reports consumed from broker. Size of this table (i.e. number of records) scales linearly with the number of clusters, because only latest report for given cluster is stored (it is guarantied by DB constraints). That table has defined compound key org_id+cluster, additionally cluster name needs to be unique across all organizations. Additionally kafka_offset is used to speedup consuming messages from Kafka topic in case the offset is lost due to issues in Kafka, Kafka library, or the service itself (messages with lower offset are skipped): . CREATE TABLE report ( org_id INTEGER NOT NULL, cluster VARCHAR NOT NULL UNIQUE, report VARCHAR NOT NULL, reported_at TIMESTAMP, last_checked_at TIMESTAMP, kafka_offset BIGINT NOT NULL DEFAULT 0, PRIMARY KEY(org_id, cluster) ) . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/db_structure.html#table-report",
    "relUrl": "/db_structure.html#table-report"
  },"15": {
    "doc": "DB structure",
    "title": "Tables rule and rule_error_key",
    "content": "These tables represent the content for Insights rules to be displayed by OCM. The table rule represents more general information about the rule, whereas the rule_error_key contains information about the specific type of error which occurred. The combination of these two create an unique rule. Very trivialized example could be: . | rule “REQUIREMENTS_CHECK” . | error_key “REQUIREMENTS_CHECK_LOW_MEMORY” | error_key “REQUIREMENTS_CHECK_MISSING_SYSTEM_PACKAGE” | . | . CREATE TABLE rule ( module VARCHAR PRIMARY KEY, name VARCHAR NOT NULL, summary VARCHAR NOT NULL, reason VARCHAR NOT NULL, resolution VARCHAR NOT NULL, more_info VARCHAR NOT NULL ) . CREATE TABLE rule_error_key ( error_key VARCHAR NOT NULL, rule_module VARCHAR NOT NULL REFERENCES rule(module), condition VARCHAR NOT NULL, description VARCHAR NOT NULL, impact INTEGER NOT NULL, likelihood INTEGER NOT NULL, publish_date TIMESTAMP NOT NULL, active BOOLEAN NOT NULL, generic VARCHAR NOT NULL, tags VARCHAR NOT NULL DEFAULT '', PRIMARY KEY(error_key, rule_module) ) . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/db_structure.html#tables-rule-and-rule_error_key",
    "relUrl": "/db_structure.html#tables-rule-and-rule_error_key"
  },"16": {
    "doc": "DB structure",
    "title": "Table cluster_rule_user_feedback",
    "content": "-- user_vote is user's vote, -- 0 is none, -- 1 is like, -- -1 is dislike CREATE TABLE cluster_rule_user_feedback ( cluster_id VARCHAR NOT NULL, rule_id VARCHAR NOT NULL, user_id VARCHAR NOT NULL, message VARCHAR NOT NULL, user_vote SMALLINT NOT NULL, added_at TIMESTAMP NOT NULL, updated_at TIMESTAMP NOT NULL, PRIMARY KEY(cluster_id, rule_id, user_id), FOREIGN KEY (cluster_id) REFERENCES report(cluster) ON DELETE CASCADE, FOREIGN KEY (rule_id) REFERENCES rule(module) ON DELETE CASCADE ) . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/db_structure.html#table-cluster_rule_user_feedback",
    "relUrl": "/db_structure.html#table-cluster_rule_user_feedback"
  },"17": {
    "doc": "DB structure",
    "title": "Table cluster_rule_toggle",
    "content": "CREATE TABLE cluster_rule_toggle ( cluster_id VARCHAR NOT NULL, rule_id VARCHAR NOT NULL, user_id VARCHAR NOT NULL, disabled SMALLINT NOT NULL, disabled_at TIMESTAMP NULL, enabled_at TIMESTAMP NULL, updated_at TIMESTAMP NOT NULL, CHECK (disabled &gt;= 0 AND disabled &lt;= 1), PRIMARY KEY(cluster_id, rule_id, user_id) ) . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/db_structure.html#table-cluster_rule_toggle",
    "relUrl": "/db_structure.html#table-cluster_rule_toggle"
  },"18": {
    "doc": "DB structure",
    "title": "Table consumer_error",
    "content": "Errors that happen while processing a message consumed from Kafka are logged into this table. This allows easier debugging of various issues, especially those related to unexpected input data format. CREATE TABLE consumer_error ( topic VARCHAR NOT NULL, partition INTEGER NOT NULL, topic_offset INTEGER NOT NULL, key VARCHAR, produced_at TIMESTAMP NOT NULL, consumed_at TIMESTAMP NOT NULL, message VARCHAR, error VARCHAR NOT NULL, PRIMARY KEY(topic, partition, topic_offset) ) . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/db_structure.html#table-consumer_error",
    "relUrl": "/db_structure.html#table-consumer_error"
  },"19": {
    "doc": "Documentation for developers",
    "title": "Documentation for developers",
    "content": "All packages developed in this project have documentation available on GoDoc server: . | entry point to the service | package broker | package consumer | package content | package logger | package metrics | package migration | package producer | package server | package storage | package types | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/documentation_for_developers.html",
    "relUrl": "/documentation_for_developers.html"
  },"20": {
    "doc": "Description",
    "title": "Description",
    "content": "Insights Results Aggregator is a service that provides Insight OCP data that are being consumed by OpenShift Cluster Manager. That data contain information about clusters status (especially health, security, performance) based on results generated by Insights rules engine. Insights OCP data are consumed from selected broker, stored in a storage (that basically works as a cache) and exposed via REST API endpoints. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/",
    "relUrl": "/"
  },"21": {
    "doc": "Description",
    "title": "Database description",
    "content": ". | Database description is available here | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/#database-description",
    "relUrl": "/#database-description"
  },"22": {
    "doc": "Description",
    "title": "Documentation for source files from this repository",
    "content": ". | aggregator.go | consumer.go | server.go | server/rules.go | server/auth_test.go | server/router_utils_test.go | server/export_test.go | server/endpoints.go | server/report_benchmarks_test.go | server/server_read_report_test.go | server/vote_endpoints_benchmarks_test.go | server/server.go | server/auth.go | server/router_utils.go | server/server_test.go | server/configuration.go | server/errors.go | server/vote_endpoints.go | conf/export_test.go | conf/configuration_test.go | conf/configuration.go | aggregator_test.go | export_test.go | migration/mig_0007_create_cluster_rule_toggle.go | migration/mig_0004_modify_cluster_rule_user_feedback.go | migration/mig_0003_create_cluster_rule_user_feedback.go | migration/helpers.go | migration/mig_0008_add_offset_field_to_report_table.go | migration/actual_migrations_test.go | migration/mig_0011_remove_fk_and_content_tables.go | migration/mig_0009_add_index_on_report_kafka_offset.go | migration/export_test.go | migration/mig_0006_add_on_delete_cascade.go | migration/migration_test.go | migration/migrations.go | migration/mig_0001_create_report.go | migration/sql.go | migration/mig_0005_create_consumer_error.go | migration/mig_0010_add_tags_on_rule_content.go | migration/migration.go | migration/mig_0002_create_rule_content.go | types/types.go | types/pg_consts.go | types/errors.go | consumer/logging.go | consumer/benchmark_test.go | consumer/export_test.go | consumer/consumer_test.go | consumer/processing.go | consumer/consumer.go | consumer/consts.go | producer/producer.go | producer/producer_test.go | storage/storage.go | storage/storage_rules_test.go | storage/sql_hooks.go | storage/noop_storage.go | storage/sql_hooks_test.go | storage/export_test.go | storage/rule_feedback.go | storage/rule_toggle.go | storage/storage_test.go | storage/configuration.go | storage/generic_insert_benchmark_test.go | storage/noop_storage_test.go | logger/configuration.go | logger/logger.go | logger/logger_test.go | metrics/metrics_test.go | metrics/metrics.go | run_main_test.go | broker/configuration.go | tests/rest/openapi.go | tests/rest/rest.go | tests/rest/reports.go | tests/rest/organizations.go | tests/rest/common.go | tests/rest/org_clusters.go | tests/rest/entrypoint.go | tests/rest/metrics.go | tests/rest/rule_vote.go | tests/helpers/helpers.go | tests/helpers/mock_consumer.go | tests/helpers/mock_consumer_group_session.go | tests/helpers/http.go | tests/helpers/consumer.go | tests/helpers/mock_consumer_group_claim.go | tests/helpers/mock_storage.go | tests/testdata/testdata.go | tests/rest_api_tests.go | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/#documentation-for-source-files-from-this-repository",
    "relUrl": "/#documentation-for-source-files-from-this-repository"
  },"23": {
    "doc": "Description",
    "title": "Documentation for utilities from this repository",
    "content": ". | utils/json_check.py | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/#documentation-for-utilities-from-this-repository",
    "relUrl": "/#documentation-for-utilities-from-this-repository"
  },"24": {
    "doc": "Local setup",
    "title": "Local setup",
    "content": " ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/local_setup.html",
    "relUrl": "/local_setup.html"
  },"25": {
    "doc": "Local setup",
    "title": "Table of contents",
    "content": ". | Prerequisites | Usage | Kafka producer | . There is a docker-compose configuration that provisions a minimal stack of Insight Platform and a postgres database. You can download it here https://gitlab.cee.redhat.com/insights-qe/iqe-ccx-plugin/blob/master/docker-compose.yml . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/local_setup.html#table-of-contents",
    "relUrl": "/local_setup.html#table-of-contents"
  },"26": {
    "doc": "Local setup",
    "title": "Prerequisites",
    "content": ". | edit localhost line in your /etc/hosts: 127.0.0.1 localhost kafka minio | ingress image should present on your machine. You can build it locally from this repo | . https://github.com/RedHatInsights/insights-ingress-go . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/local_setup.html#prerequisites",
    "relUrl": "/local_setup.html#prerequisites"
  },"27": {
    "doc": "Local setup",
    "title": "Usage",
    "content": ". | Start the stack podman-compose up or docker-compose up | Wait until kafka will be up. | Start ccx-data-pipeline: python3 -m insights_messaging config-devel.yaml | Build insights-results-aggregator: make build | Start insights-results-aggregator: INSIGHTS_RESULTS_AGGREGATOR_CONFIG_FILE=config-devel.toml ./insights-results-aggregator | . Stop Minimal Insights Platform stack podman-compose down or docker-compose down . In order to upload an insights archive, you can use curl: . curl -k -vvvv -F \"upload=@/path/to/your/archive.zip;type=application/vnd.redhat.testareno.archive+zip\" http://localhost:3000/api/ingress/v1/upload -H \"x-rh-identity: eyJpZGVudGl0eSI6IHsiYWNjb3VudF9udW1iZXIiOiAiMDAwMDAwMSIsICJpbnRlcm5hbCI6IHsib3JnX2lkIjogIjEifX19Cg==\" . or you can use integration tests suite. More details are here. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/local_setup.html#usage",
    "relUrl": "/local_setup.html#usage"
  },"28": {
    "doc": "Local setup",
    "title": "Kafka producer",
    "content": "It is possible to use the script produce_insights_results from utils to produce several Insights results into Kafka topic. Its dependency is Kafkacat that needs to be installed on the same machine. You can find installation instructions on this page. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/local_setup.html#kafka-producer",
    "relUrl": "/local_setup.html#kafka-producer"
  },"29": {
    "doc": "pprof",
    "title": "pprof",
    "content": "In debug mode, standard Golang pprof interface is available at /debug/pprof/ . Common usage (for using pprof against local instance): . go tool pprof localhost:8080/debug/pprof/profile . A practical example is available here: https://medium.com/@paulborile/profiling-a-golang-rest-api-server-635fa0ed45f3 . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/pprof.html",
    "relUrl": "/pprof.html"
  },"30": {
    "doc": "Prometheus API",
    "title": "Prometheus API",
    "content": "It is possible to use /api/v1/metrics REST API endpoint to read all metrics exposed to Prometheus or to any tool that is compatible with it. Currently, the following metrics are exposed: . | consumed_messages the total number of messages consumed from Kafka | consuming_errors the total number of errors during consuming messages from Kafka | successful_messages_processing_time the time to process successfully message | failed_messages_processing_time the time to process message fail | last_checked_timestamp_lag_minutes shows how slow we get messages from clusters | produced_messages the total number of produced messages sent to Payload Tracker’s Kafka topic | written_reports the total number of reports written to the storage | feedback_on_rules the total number of left feedback | sql_queries_counter the total number of SQL queries | sql_queries_durations the SQL queries durations | . Additionally it is possible to consume all metrics provided by Go runtime. There metrics start with go_ and process_ prefixes. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/prometheus.html",
    "relUrl": "/prometheus.html"
  },"31": {
    "doc": "Prometheus API",
    "title": "API related metrics",
    "content": "There are a set of metrics provieded by insights-operator-utils library, all of them related with the API usage. These are the API metrics exposed: . | api_endpoints_requests the total number of requests per endpoint | api_endpoints_response_time API endpoints response time | api_endpoints_status_codes a counter of the HTTP status code responses returned back by the service | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/prometheus.html#api-related-metrics",
    "relUrl": "/prometheus.html#api-related-metrics"
  },"32": {
    "doc": "Prometheus API",
    "title": "Metrics namespace",
    "content": "As explained in the configuration section of this documentation, a namespace can be provided in order to act as a prefix to the metric name. If no namespace is provided in the configuration, the metrics will be exposed as described in this documentation. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/prometheus.html#metrics-namespace",
    "relUrl": "/prometheus.html#metrics-namespace"
  },"33": {
    "doc": "References",
    "title": "References",
    "content": ". | Insights Data Schemas | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/references.html",
    "relUrl": "/references.html"
  },"34": {
    "doc": "REST API",
    "title": "REST API",
    "content": "Aggregator service provides information about its REST API schema via endpoint api/v1/openapi.json. OpenAPI 3.0 is used to describe the schema; it can be read by human and consumed by computers. For example, if aggregator is started locally, it is possible to read schema based on OpenAPI 3.0 specification by using the following command: . curl localhost:8080/api/v1/openapi.json . Please note that OpenAPI schema is accessible w/o the need to provide authorization tokens. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/rest_api.html",
    "relUrl": "/rest_api.html"
  },"35": {
    "doc": "REST API",
    "title": "Accessing results",
    "content": "Settings for localhost . export ADDRESS=localhost:8080/api/v1 . Basic endpoints . List of clusters associated with the specified organization ID . /organizations/{orgId}/clusters . Usage: . curl -k -v $ADDRESS/organizations/{orgId}/clusters . Report for the given organization and cluster . /organizations/{orgId}/clusters/{clusterId}/users/{userId}/report . Usage: . curl -k -v $ADDRESS/organizations/{orgId}/clusters/{clusterId}/users/{userId}/report . Latest reports for the given list of clusters . Using GET method . /organizations/{orgId}/clusters/{clusterList}/reports . Usage: . curl -k -v $ADDRESS/organizations/{orgId}/clusters/{cluster1}/report curl -k -v $ADDRESS/organizations/{orgId}/clusters/{cluster1},{cluster2}/report curl -k -v $ADDRESS/organizations/{orgId}/clusters/{cluster1},{cluster2},{cluster3}/report . Plase note that the total URL length is limited, usually to 2000 or 2048 characters. Using POST method . /organizations/{orgId}/clusters/reports . Usage: . curl -k -v $ADDRESS/organizations/{orgId}/clusters/reports -d @cluster_list.json . Format of the payload: . { \"clusters\" : [ \"34c3ecc5-624a-49a5-bab8-4fdc5e51a266\", \"74ae54aa-6577-4e80-85e7-697cb646ff37\", \"a7467445-8d6a-43cc-b82c-7007664bdf69\", \"ee7d2bf4-8933-4a3a-8634-3328fe806e08\" ] } . Latest rule report for the given organization, cluster, user and rule ids . /organizations/{orgId}/clusters/{clusterId}/users/{userId}/rules/{ruleId} . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/rest_api.html#accessing-results",
    "relUrl": "/rest_api.html#accessing-results"
  },"36": {
    "doc": "Rules",
    "title": "Rules",
    "content": "The user has the ability to disable a rule/health check recommendation that they’re not interested in to stop it from showing in OCM. The user also has the ability to re-enable the rule, in case they later become interested in it, or in the case of an accidental disable, for example. This is made possible by using these two endpoints: clusters/{cluster}/rules/{rule_id}/disable clusters/{cluster}/rules/{rule_id}/enable . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/rules.html",
    "relUrl": "/rules.html"
  },"37": {
    "doc": "Rules",
    "title": "Tutorial rule",
    "content": "Directory rules/tutorial/ contains tutorial rule that is ‘hit’ by any cluster. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/rules.html#tutorial-rule",
    "relUrl": "/rules.html#tutorial-rule"
  },"38": {
    "doc": "Rules",
    "title": "Rules content",
    "content": "The generated cluster reports from Insights results contain three lists of rules that were either skipped (because of missing requirements, etc.), passed (the rule got executed but no issue was found), or hit (the rule got executed and found the issue it was looking for) by the cluster, where each rule is represented as a dictionary containing identifying information about the rule itself. The hit rules are the rules that the customer is interested in and therefore the information about them (their content) needs to be displayed in OCM. The content for the rules is present in another repository, alongside the actual rule implementations, which is primarily maintained by the support-facing team. For that reason, the insights-results-aggregator is setup in a way, that the content we’re interested in is copied to the Docker image from the repository during image build and we have a push webhook on master branch set up in that repository, signalling our app to be rebuilt. This content is then processed upon application start-up, correctly parsed by the package content and saved into the database (see DB structure). When a request for a cluster report comes from OCM, the report is parsed (TODO: parse reports only once when consuming them) and content for all the hit rules is returned. Local environment with rules content . The rules content parser is configured by default to expect the content in a root directory /rules-content. This can be changed either by an environment variable INSIGHTS_RESULTS_AGGREGATOR__CONTENT__PATH or by modifying the config file entry: . [content] path = \"/rules-content\" . To get the latest rules content locally, you can make rules_content, which just runs the script update_rules_content.sh mimicking the Dockerfile behavior (NOTE: you need to be in RH VPN to be able to access that repository, but it is not private). The script copies the content into a .gitignored folder rules-content, so all that’s necessary is to change the expected path. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/rules.html#rules-content",
    "relUrl": "/rules.html#rules-content"
  },"39": {
    "doc": "Testing",
    "title": "Testing",
    "content": " ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/testing.html",
    "relUrl": "/testing.html"
  },"40": {
    "doc": "Testing",
    "title": "Table of contents",
    "content": ". | Unit tests | All integration tests . | Only REST API tests | Coverage reports | . | OpenAPI checks | . tl;dr: make before_commit will run most of the checks by magic, VERBOSE=true make before_commit will do the same but print more information about what it’s doing . The following tests can be run to test your code in insights-results-aggregator. Detailed information about each type of test is included in the corresponding subsection: . | Unit tests: checks behavior of all units in source code (methods, functions) | REST API Tests: test the real REST API of locally deployed application with database initialized with test data only | Integration tests: the integration tests for insights-results-aggregator service | Metrics tests: test whether Prometheus metrics are exposed as expected | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/testing.html#table-of-contents",
    "relUrl": "/testing.html#table-of-contents"
  },"41": {
    "doc": "Testing",
    "title": "Unit tests",
    "content": "Set of unit tests checks all units of source code. Additionally the code coverage is computed and displayed. Code coverage is stored in a file coverage.out and can be checked by a script named check_coverage.sh. To run unit tests use the following command: . make test . If you have postgres running on port from ./config-devel.toml file it will also run tests against it . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/testing.html#unit-tests",
    "relUrl": "/testing.html#unit-tests"
  },"42": {
    "doc": "Testing",
    "title": "All integration tests",
    "content": "make integration_tests . Only REST API tests . Set of tests to check REST API of locally deployed application with database initialized with test data only. To run REST API tests use the following command: . make rest_api_tests . By default all logs from the application aren’t shown, if you want to see them, run: ./test.sh rest_api --verbose . Coverage reports . To make a coverage report you need to start ./make-coverage.sh tool with one of these arguments: . | unit-sqlite unit tests with sqlite in memory database | unit-posgres unit tests with postgres database(don’t forget to start docker-compose up with the DB) | rest REST API tests from test.sh file | integration Any external tests, for example from iqe-ccx-plugin. Only this option requires you to run tests manually and stop the script by Ctrl+C when they are done | . For example: ./make-coverage.sh unit-sqlite will generate a report file coverage.out which you can investigate by either of those commands: . | go tool cover -func=coverage.out | go tool cover -html=coverage.out if your system supports it, this command will open a browser with a nice colored report | . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/testing.html#all-integration-tests",
    "relUrl": "/testing.html#all-integration-tests"
  },"43": {
    "doc": "Testing",
    "title": "OpenAPI checks",
    "content": "Content of file openapi.json can be checked by using: . make openapi-check . It is also possible to use online checker that is available on https://apitools.dev/swagger-parser/online/ . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/testing.html#openapi-checks",
    "relUrl": "/testing.html#openapi-checks"
  },"44": {
    "doc": "Utilities",
    "title": "Utilities",
    "content": "Utilities are stored in utils subdirectory. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/utilities.html",
    "relUrl": "/utilities.html"
  },"45": {
    "doc": "Utilities",
    "title": "json_check.py",
    "content": "Simple checker if all JSONs have the correct syntax (not scheme). Script usage . usage: json_check.py [-h] [-v] optional arguments: -h, --help show this help message and exit -v, --verbose make it verbose -n, --no-colors disable color output -d DIRECTORY, --directory DIRECTORY directory with JSON files to check . Annotated source code . ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/utilities.html#json_checkpy",
    "relUrl": "/utilities.html#json_checkpy"
  },"46": {
    "doc": "Utilities",
    "title": "docgo.sh",
    "content": "Generates documentation for all packages. Please note that other utilities and tools have in available in separate repository RedHatInsights / insights-results-aggregator-utils. More detailed description of these tools is available there. ",
    "url": "https://redhatinsights.github.io/insights-results-aggregator/utilities.html#docgosh",
    "relUrl": "/utilities.html#docgosh"
  }
}
